/home/ubuntu/ART/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
05:39:56 [INFO] benchmark: [sglang] Worker PID=83453 GPUs=4
05:40:12 [INFO] benchmarks.sglang_vs_vllm.sglang_server: Starting SGLang (verl-style, will NOT restart): /home/ubuntu/.venvs/sglang-bench/bin/python -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B-Instruct-2507 --served-model-name Qwen/Qwen3-30B-A3B-Instruct-2507 --port 8200 --host 0.0.0.0 --tp 2 --mem-fraction-static 0.7 --max-running-requests 256 --dtype auto --chunked-prefill-size 32768 --trust-remote-code --enable-p2p-check --enable-memory-saver --enable-lora --max-lora-rank 8 --lora-target-modules q_proj k_proj v_proj o_proj gate_proj up_proj down_proj
05:41:05 [INFO] benchmarks.sglang_vs_vllm.sglang_server: SGLang ready in 52.88s (pid=83598) — will stay alive for all steps
05:41:05 [INFO] benchmarks.sglang_vs_vllm.sglang_megatron_service: SGLang ready (verl-style, persistent) — serving Qwen/Qwen3-30B-A3B-Instruct-2507 on port 8200
05:41:05 [INFO] benchmark: [sglang] ready in 53s — Qwen/Qwen3-30B-A3B-Instruct-2507 @ http://0.0.0.0:8200/v1 (verl-style, will NOT restart)
05:41:06 [INFO] benchmark: [sglang] step 1/3 (verl-style)
05:41:23 [INFO] benchmark:   rollout 16.8s  581 tok/s  TTFT=0.3226s  err=0
train:   0%|          | 0/3 [00:00<?, ?it/s]05:41:32 [INFO] benchmarks.sglang_vs_vllm.sglang_server: SGLang sleep (release memory) in 0.19s — tags=['kv_cache', 'weights']
05:41:32 [INFO] benchmarks.sglang_vs_vllm.sglang_megatron_service: SGLang sleeping (kv_cache + weights released) in 0.19s
05:41:32 [INFO] benchmarks.sglang_vs_vllm.sglang_megatron_service: Phase 1 — sleep(kv_cache+weights): 0.19s
05:41:33 [INFO] megatron.core.msc_utils: The multistorageclient package is available.
05:41:34 [INFO] megatron.core.distributed.fsdp.src.megatron_fsdp.megatron_fsdp: Detected Megatron Core, using Megatron-FSDP with Megatron.
05:41:34 [INFO] megatron.core.distributed.fsdp.src.megatron_fsdp.param_and_grad_buffer: Detected Megatron Core, using Megatron-FSDP with Megatron.
05:41:34 [WARNING] megatron.core.utils: fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
train:  33%|███▎      | 1/3 [00:06<00:12,  6.23s/it]train:  33%|███▎      | 1/3 [00:06<00:12,  6.23s/it, loss=0.229, grad_norm=12.1, probs_corr=nan]/home/ubuntu/ART/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
W0210 05:41:40.174000 84437 torch/distributed/run.py:803] 
W0210 05:41:40.174000 84437 torch/distributed/run.py:803] *****************************************
W0210 05:41:40.174000 84437 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0210 05:41:40.174000 84437 torch/distributed/run.py:803] *****************************************
/home/ubuntu/ART/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ubuntu/ART/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ubuntu/ART/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ubuntu/ART/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
/home/ubuntu/ART/.venv/lib/python3.11/site-packages/megatron/core/models/gpt/gpt_layer_specs.py:103: UserWarning: The fp8 argument in "get_gpt_layer_with_transformer_engine_spec" has been deprecated and will be removed soon. Please update your code accordingly.
  warnings.warn(
/home/ubuntu/ART/.venv/lib/python3.11/site-packages/megatron/core/models/gpt/gpt_layer_specs.py:103: UserWarning: The fp8 argument in "get_gpt_layer_with_transformer_engine_spec" has been deprecated and will be removed soon. Please update your code accordingly.
  warnings.warn(
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]Fetching 17 files: 100%|██████████| 17/17 [00:00<00:00, 105948.24it/s]
Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]Fetching 17 files: 100%|██████████| 17/17 [00:00<00:00, 99032.18it/s]
/home/ubuntu/ART/.venv/lib/python3.11/site-packages/megatron/core/models/gpt/gpt_layer_specs.py:103: UserWarning: The fp8 argument in "get_gpt_layer_with_transformer_engine_spec" has been deprecated and will be removed soon. Please update your code accordingly.
  warnings.warn(
/home/ubuntu/ART/.venv/lib/python3.11/site-packages/megatron/core/models/gpt/gpt_layer_specs.py:103: UserWarning: The fp8 argument in "get_gpt_layer_with_transformer_engine_spec" has been deprecated and will be removed soon. Please update your code accordingly.
  warnings.warn(
Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]Fetching 17 files: 100%|██████████| 17/17 [00:00<00:00, 125093.28it/s]
Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]Fetching 17 files: 100%|██████████| 17/17 [00:00<00:00, 90601.23it/s]
/home/ubuntu/ART/.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
/home/ubuntu/ART/src/art/loss.py:39: UserWarning: cov(): degrees of freedom is <= 0. Correction should be strictly less than the number of observations. (Triggered internally at /pytorch/aten/src/ATen/native/Correlation.cpp:116.)
  probs_corr = torch.corrcoef(
/home/ubuntu/ART/src/art/loss.py:39: UserWarning: cov(): degrees of freedom is <= 0. Correction should be strictly less than the number of observations. (Triggered internally at /pytorch/aten/src/ATen/native/Correlation.cpp:116.)
  probs_corr = torch.corrcoef(
/home/ubuntu/ART/src/art/loss.py:39: UserWarning: cov(): degrees of freedom is <= 0. Correction should be strictly less than the number of observations. (Triggered internally at /pytorch/aten/src/ATen/native/Correlation.cpp:116.)
  probs_corr = torch.corrcoef(
/home/ubuntu/ART/src/art/loss.py:39: UserWarning: cov(): degrees of freedom is <= 0. Correction should be strictly less than the number of observations. (Triggered internally at /pytorch/aten/src/ATen/native/Correlation.cpp:116.)
  probs_corr = torch.corrcoef(
train:  67%|██████▋   | 2/3 [01:28<00:51, 51.06s/it, loss=0.229, grad_norm=12.1, probs_corr=nan]train:  67%|██████▋   | 2/3 [01:28<00:51, 51.06s/it, loss=-0.624, grad_norm=5.47, probs_corr=nan]/home/ubuntu/ART/src/art/loss.py:39: UserWarning: cov(): degrees of freedom is <= 0. Correction should be strictly less than the number of observations. (Triggered internally at /pytorch/aten/src/ATen/native/Correlation.cpp:116.)
  probs_corr = torch.corrcoef(
/home/ubuntu/ART/src/art/loss.py:39: UserWarning: cov(): degrees of freedom is <= 0. Correction should be strictly less than the number of observations. (Triggered internally at /pytorch/aten/src/ATen/native/Correlation.cpp:116.)
  probs_corr = torch.corrcoef(
/home/ubuntu/ART/src/art/loss.py:39: UserWarning: cov(): degrees of freedom is <= 0. Correction should be strictly less than the number of observations. (Triggered internally at /pytorch/aten/src/ATen/native/Correlation.cpp:116.)
  probs_corr = torch.corrcoef(
/home/ubuntu/ART/src/art/loss.py:39: UserWarning: cov(): degrees of freedom is <= 0. Correction should be strictly less than the number of observations. (Triggered internally at /pytorch/aten/src/ATen/native/Correlation.cpp:116.)
  probs_corr = torch.corrcoef(
train: 100%|██████████| 3/3 [01:42<00:00, 33.92s/it, loss=-0.624, grad_norm=5.47, probs_corr=nan]train: 100%|██████████| 3/3 [01:42<00:00, 33.92s/it, loss=0.462, grad_norm=3.94, probs_corr=nan] /home/ubuntu/ART/.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
05:43:26 [INFO] benchmarks.sglang_vs_vllm.sglang_server: SGLang wake_up (resume memory) in 0.10s — tags=['kv_cache', 'weights']
05:43:26 [INFO] benchmarks.sglang_vs_vllm.sglang_megatron_service: SGLang awake (kv_cache + weights resumed) in 0.10s
05:43:26 [INFO] benchmarks.sglang_vs_vllm.sglang_megatron_service: Phase 4 — wake_up(kv_cache+weights): 0.10s
05:43:28 [INFO] benchmarks.sglang_vs_vllm.sglang_server: LoRA adapter 'bench-sglang@step1' loaded in 1.56s
05:43:28 [INFO] benchmarks.sglang_vs_vllm.sglang_megatron_service: LoRA hot-reload: 'bench-sglang@step1' loaded in 1.56s (was 464s with disk merge)
05:43:28 [INFO] benchmarks.sglang_vs_vllm.sglang_megatron_service: Phase 5 — _hot_reload_lora: 1.56s
05:43:28 [INFO] benchmarks.sglang_vs_vllm.sglang_megatron_service: Total transition overhead: 115.63s (was 464s+ with disk merge)
train: 100%|██████████| 3/3 [01:55<00:00, 38.54s/it, loss=0.462, grad_norm=3.94, probs_corr=nan]
05:43:28 [INFO] benchmark:   train step=1 loss=0.022344162066777546
05:43:28 [INFO] benchmark: [sglang] step 2/3 (verl-style)
05:43:39 [INFO] benchmark:   rollout 10.8s  1512 tok/s  TTFT=0.1928s  err=0
train:   0%|          | 0/3 [00:00<?, ?it/s]05:43:44 [INFO] benchmarks.sglang_vs_vllm.sglang_server: SGLang sleep (release memory) in 0.17s — tags=['kv_cache', 'weights']
05:43:44 [INFO] benchmarks.sglang_vs_vllm.sglang_megatron_service: SGLang sleeping (kv_cache + weights released) in 0.17s
05:43:44 [INFO] benchmarks.sglang_vs_vllm.sglang_megatron_service: Phase 1 — sleep(kv_cache+weights): 0.17s
train:  33%|███▎      | 1/3 [00:17<00:35, 17.92s/it]train:  33%|███▎      | 1/3 [00:17<00:35, 17.92s/it, loss=0.055, grad_norm=3.12, probs_corr=nan]train:  67%|██████▋   | 2/3 [00:31<00:15, 15.46s/it, loss=0.055, grad_norm=3.12, probs_corr=nan]train:  67%|██████▋   | 2/3 [00:31<00:15, 15.46s/it, loss=-0.999, grad_norm=5.06, probs_corr=nan]05:44:30 [INFO] benchmarks.sglang_vs_vllm.sglang_server: SGLang wake_up (resume memory) in 0.13s — tags=['kv_cache', 'weights']
05:44:30 [INFO] benchmarks.sglang_vs_vllm.sglang_megatron_service: SGLang awake (kv_cache + weights resumed) in 0.13s
05:44:30 [INFO] benchmarks.sglang_vs_vllm.sglang_megatron_service: Phase 4 — wake_up(kv_cache+weights): 0.13s
05:44:32 [INFO] benchmarks.sglang_vs_vllm.sglang_server: LoRA adapter 'bench-sglang@step2' loaded in 2.45s
05:44:32 [INFO] benchmarks.sglang_vs_vllm.sglang_megatron_service: LoRA hot-reload: 'bench-sglang@step2' loaded in 2.45s (was 464s with disk merge)
05:44:32 [INFO] benchmarks.sglang_vs_vllm.sglang_megatron_service: Phase 5 — _hot_reload_lora: 2.45s
05:44:32 [INFO] benchmarks.sglang_vs_vllm.sglang_megatron_service: Total transition overhead: 48.44s (was 464s+ with disk merge)
train:  67%|██████▋   | 2/3 [00:48<00:24, 24.22s/it, loss=-0.999, grad_norm=5.06, probs_corr=nan]
05:44:32 [INFO] benchmark:   train step=2 loss=-0.47183957695961
05:44:32 [INFO] benchmark: [sglang] step 3/3 (verl-style)
05:44:43 [INFO] benchmark:   rollout 10.8s  1428 tok/s  TTFT=0.1856s  err=0
train:   0%|          | 0/3 [00:00<?, ?it/s]05:44:49 [INFO] benchmarks.sglang_vs_vllm.sglang_server: SGLang sleep (release memory) in 0.16s — tags=['kv_cache', 'weights']
05:44:49 [INFO] benchmarks.sglang_vs_vllm.sglang_megatron_service: SGLang sleeping (kv_cache + weights released) in 0.16s
05:44:49 [INFO] benchmarks.sglang_vs_vllm.sglang_megatron_service: Phase 1 — sleep(kv_cache+weights): 0.16s
train:  33%|███▎      | 1/3 [00:18<00:36, 18.21s/it]train:  33%|███▎      | 1/3 [00:18<00:36, 18.21s/it, loss=-0.713, grad_norm=3.92, probs_corr=nan]train:  67%|██████▋   | 2/3 [00:31<00:15, 15.46s/it, loss=-0.713, grad_norm=3.92, probs_corr=nan]train:  67%|██████▋   | 2/3 [00:31<00:15, 15.46s/it, loss=-0.0465, grad_norm=4.51, probs_corr=nan]05:45:34 [INFO] benchmarks.sglang_vs_vllm.sglang_server: SGLang wake_up (resume memory) in 0.12s — tags=['kv_cache', 'weights']
05:45:34 [INFO] benchmarks.sglang_vs_vllm.sglang_megatron_service: SGLang awake (kv_cache + weights resumed) in 0.12s
05:45:34 [INFO] benchmarks.sglang_vs_vllm.sglang_megatron_service: Phase 4 — wake_up(kv_cache+weights): 0.12s
05:45:36 [INFO] benchmarks.sglang_vs_vllm.sglang_server: LoRA adapter 'bench-sglang@step3' loaded in 1.63s
05:45:36 [INFO] benchmarks.sglang_vs_vllm.sglang_megatron_service: LoRA hot-reload: 'bench-sglang@step3' loaded in 1.63s (was 464s with disk merge)
05:45:36 [INFO] benchmarks.sglang_vs_vllm.sglang_megatron_service: Phase 5 — _hot_reload_lora: 1.63s
05:45:36 [INFO] benchmarks.sglang_vs_vllm.sglang_megatron_service: Total transition overhead: 47.26s (was 464s+ with disk merge)
train:  67%|██████▋   | 2/3 [00:47<00:23, 23.63s/it, loss=-0.0465, grad_norm=4.51, probs_corr=nan]
05:45:36 [INFO] benchmark:   train step=3 loss=-0.3797023594379425
05:45:36 [INFO] benchmark: [sglang] Results → benchmark_results/sglang_metrics.json
[W210 05:45:38.345835890 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
/home/ubuntu/.local/share/uv/python/cpython-3.11.14-linux-x86_64-gnu/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/home/ubuntu/.local/share/uv/python/cpython-3.11.14-linux-x86_64-gnu/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/home/ubuntu/.local/share/uv/python/cpython-3.11.14-linux-x86_64-gnu/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/home/ubuntu/.local/share/uv/python/cpython-3.11.14-linux-x86_64-gnu/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
