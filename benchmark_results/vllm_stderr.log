/home/ubuntu/ART/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
20:01:21 [INFO] benchmark: [vllm] Worker PID=42371 GPUs=4
20:01:28 [INFO] benchmark: [vllm] cleaned stale checkpoints at .art/sglang-vs-vllm/models
/home/ubuntu/ART/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
Process Process-1:
Traceback (most recent call last):
  File "/home/ubuntu/.local/share/uv/python/cpython-3.11.14-linux-x86_64-gnu/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/.local/share/uv/python/cpython-3.11.14-linux-x86_64-gnu/lib/python3.11/asyncio/base_events.py", line 654, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/ubuntu/ART/benchmarks/sglang_vs_vllm/run_benchmark.py", line 441, in _main
    result = await fn()
             ^^^^^^^^^^
  File "/home/ubuntu/ART/benchmarks/sglang_vs_vllm/run_benchmark.py", line 245, in _run_vllm
    await model.register(bk, _openai_client_config={
  File "/home/ubuntu/ART/src/art/model.py", line 789, in register
    base_url, api_key = await backend._prepare_backend_for_training(
Traceback (most recent call last):
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/ART/src/art/local/backend.py", line 272, in _prepare_backend_for_training
    host, port = await service.start_openai_server(config=config)
  File "/home/ubuntu/.local/share/uv/python/cpython-3.11.14-linux-x86_64-gnu/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/ART/src/mp_actors/traceback.py", line 24, in async_wrapper
  File "/home/ubuntu/.local/share/uv/python/cpython-3.11.14-linux-x86_64-gnu/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ubuntu/ART/src/mp_actors/move.py", line 257, in _target
    asyncio.run(_handle_requests(obj, requests, responses))
  File "/home/ubuntu/ART/.venv/lib/python3.11/site-packages/nest_asyncio.py", line 30, in run
    return loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    return await func(*args, **kwargs)
  File "/home/ubuntu/ART/.venv/lib/python3.11/site-packages/nest_asyncio.py", line 92, in run_until_complete
    self._run_once()
  File "/home/ubuntu/ART/.venv/lib/python3.11/site-packages/nest_asyncio.py", line 133, in _run_once
    handle._run()
  File "/home/ubuntu/.local/share/uv/python/cpython-3.11.14-linux-x86_64-gnu/lib/python3.11/asyncio/events.py", line 84, in _run
    self._context.run(self._callback, *self._args)
  File "/home/ubuntu/.local/share/uv/python/cpython-3.11.14-linux-x86_64-gnu/lib/python3.11/asyncio/tasks.py", line 277, in __step
    result = coro.send(None)
             ^^^^^^^^^^^^^^^
  File "/home/ubuntu/ART/src/mp_actors/move.py", line 288, in _handle_request
    result = await result_or_coro
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/ART/src/art/megatron/service.py", line 203, in start_openai_server
    self._ensure_identity_lora(lora_path)
  File "/home/ubuntu/ART/src/art/megatron/service.py", line 121, in _ensure_identity_lora
    self._create_identity_lora(lora_path)
  File "/home/ubuntu/ART/src/art/megatron/service.py", line 100, in _create_identity_lora
    model = AutoModelForCausalLM.from_pretrained(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/ART/.venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 604, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/ART/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 277, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/ART/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 5029, in from_pretrained
    device_map = _get_device_map(model, device_map, max_memory, hf_quantizer, dtype, keep_in_fp32_regex)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/ART/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 1336, in _get_device_map
    inferred_max_memory = get_balanced_memory(
                          ^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/ART/.venv/lib/python3.11/site-packages/accelerate/utils/modeling.py", line 1031, in get_balanced_memory
    module_sizes = {n: v for n, v in module_sizes.items() if n not in leaves}
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/ART/.venv/lib/python3.11/site-packages/accelerate/utils/modeling.py", line 1031, in <dictcomp>
    module_sizes = {n: v for n, v in module_sizes.items() if n not in leaves}
                                                             ^^^^^^^^^^^^^^^
  File "/home/ubuntu/ART/src/mp_actors/move.py", line 187, in async_method_wrapper
KeyboardInterrupt
    return await get_response(args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/ART/src/mp_actors/move.py", line 157, in get_response
    done, _ = await asyncio.wait(
              ^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/.local/share/uv/python/cpython-3.11.14-linux-x86_64-gnu/lib/python3.11/asyncio/tasks.py", line 428, in wait
    return await _wait(fs, timeout, return_when, loop)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/.local/share/uv/python/cpython-3.11.14-linux-x86_64-gnu/lib/python3.11/asyncio/tasks.py", line 535, in _wait
    await waiter
  File "/home/ubuntu/.local/share/uv/python/cpython-3.11.14-linux-x86_64-gnu/lib/python3.11/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^
  File "/home/ubuntu/.local/share/uv/python/cpython-3.11.14-linux-x86_64-gnu/lib/python3.11/asyncio/futures.py", line 198, in result
    raise exc
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/ART/benchmarks/sglang_vs_vllm/run_benchmark.py", line 632, in <module>
    main()
  File "/home/ubuntu/ART/benchmarks/sglang_vs_vllm/run_benchmark.py", line 534, in main
    run_worker(args._worker, cfg, args._results)
  File "/home/ubuntu/ART/benchmarks/sglang_vs_vllm/run_benchmark.py", line 446, in run_worker
    asyncio.run(_main())
  File "/home/ubuntu/.local/share/uv/python/cpython-3.11.14-linux-x86_64-gnu/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/home/ubuntu/.local/share/uv/python/cpython-3.11.14-linux-x86_64-gnu/lib/python3.11/asyncio/runners.py", line 123, in run
    raise KeyboardInterrupt()
KeyboardInterrupt
/home/ubuntu/.local/share/uv/python/cpython-3.11.14-linux-x86_64-gnu/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
